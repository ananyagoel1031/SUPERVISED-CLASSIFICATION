{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8500187-59f5-4935-a471-46eed6f97a29",
   "metadata": {},
   "source": [
    "## ASSIGNMENT\n",
    "\n",
    "## Supervised Classification: Decision Trees, SVM, and Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0228c05d-2c65-4ed0-b208-910aaa8201f0",
   "metadata": {},
   "source": [
    "### Q1.  What is Information Gain, and how is it used in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6751b0b-6efe-48d4-bf4c-c0047c9e98d0",
   "metadata": {},
   "source": [
    "#### A1. Information Gain (IG) is a metric used in decision trees to determine the effectiveness of a feature in splitting the dataset. It measures the reduction in entropy (uncertainty) of the target variable when a feature is used for splitting.\n",
    "\n",
    "#### Information Gain is a crucial concept in decision tree algorithms. It's used to determine which attribute (or feature) is the best to split the data at each node of the tree. The goal is to maximize information gain, which effectively minimizes the entropy (or impurity) of the resulting child nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09178489-6087-49be-aead-5e4039645e6f",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Gini Impurity and Entropy? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869bb56a-dc17-451e-8d7a-97aa0daac634",
   "metadata": {},
   "source": [
    "#### A2. Gini Impurity: The internal working of Gini impurity is also somewhat similar to the working of entropy in the Decision Tree. In the Decision Tree algorithm, both are used for building the tree by splitting as per the appropriate features but there is quite a difference in the computation of both methods.Gini index is a linear measure. The range of the Gini index is [0, 0.5], where 0 indicates perfect purity and 0.5 indicates maximum impurity. Gini index is typically used in CART (Classification and Regression Trees) algorithms.\n",
    "\n",
    "#### Entropy: It measures the amount of uncertainty or randomness in a set. The range of entropy is [0, log2(C)], where c is the number of classes. The range becomes [0, 1] for binary classification. Entropy is a logarithmic measure.Entropy is typically used in ID3 and C4.5 algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff06fd21-a083-4040-b8fe-633d37fc8c1a",
   "metadata": {},
   "source": [
    "### Q3. What is Pre-Pruning in Decision Trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a637bf-15c2-477c-bd2c-b119521ca677",
   "metadata": {},
   "source": [
    "#### A3. Pre-pruning halts the growth of the decision tree during its construction to prevent it from becoming overly complex. This is achieved by setting constraints or thresholds that the tree must adhere to while splitting nodes. Common techniques include limiting the maximum depth of the tree, setting a minimum number of samples per leaf or split, and restricting the number of features considered for splitting.Pre-pruning results in a simpler tree that is less likely to overfit the training data. It is computationally efficient as it avoids unnecessary splits, making it suitable for larger datasets. However, it may prematurely stop the tree's growth, potentially missing important patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331bb79-9317-4ac8-84be-98e445b4afea",
   "metadata": {},
   "source": [
    "### Q4. :Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4950e48c-2168-43c5-85e3-21722e8cd5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "# A4.\n",
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "# Create Decision Tree classifier object\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\n",
    "# Train Decision Tree Classifier\n",
    "clf = clf.fit(X_train, y_train)\n",
    "# Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "# Model Accuracy\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f015af2f-a7b8-4531-a119-5764cae4dbf0",
   "metadata": {},
   "source": [
    "#### In this example, we first import the necessary libraries and load the Iris dataset. We then split the dataset into training and test sets using the train_test_split function. Next, we create a DecisionTreeClassifier object with the criterion set to \"gini\" and the maximum depth set to 3. We train the classifier using the fit method and make predictions on the test set using the predict method. Finally, we calculate the accuracy of the model using the accuracy_score function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2a6af-79a1-4557-92bb-e7619fd388ee",
   "metadata": {},
   "source": [
    "### Q5. What is a Support Vector Machine (SVM)? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3253ad-4fa1-468f-a9df-2adaf1c035ae",
   "metadata": {},
   "source": [
    "#### A5. Support Vector Machines (SVM) are supervised machine learning algorithms used for classification, regression, and outlier detection. The core idea of SVM is to find the optimal hyperplane that separates data points of different classes with the maximum margin. This margin is the distance between the hyperplane and the nearest data points, known as support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe9beee-d997-496d-be38-6cdad7b50def",
   "metadata": {},
   "source": [
    "### Q6.  What is the Kernel Trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2a494c-afee-4440-a497-18a29a47eacf",
   "metadata": {},
   "source": [
    "#### A6. Kernel Trick: A method to transform non-linearly separable data into a higher-dimensional space where it becomes linearly separable. Common kernels include linear, polynomial, and radial basis function (RBF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b990c-cc0f-4a23-90cc-21d29060ed96",
   "metadata": {},
   "source": [
    "### Q7.  Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe77b071-c9bb-44f3-9d58-16f56da56bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Linear Kernel: 0.98\n",
      "Accuracy with RBF Kernel: 0.98\n"
     ]
    }
   ],
   "source": [
    "# A7.\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Wine dataset\n",
    "wine = datasets.load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Standardize the dataset for better performance\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train an SVM classifier with a Linear kernel\n",
    "svm_linear = SVC(kernel='linear', random_state=42)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "y_pred_linear = svm_linear.predict(X_test)\n",
    "\n",
    "# Train an SVM classifier with an RBF kernel\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "y_pred_rbf = svm_rbf.predict(X_test)\n",
    "\n",
    "# Calculate and compare accuracies\n",
    "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
    "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "\n",
    "print(f\"Accuracy with Linear Kernel: {accuracy_linear:.2f}\")\n",
    "print(f\"Accuracy with RBF Kernel: {accuracy_rbf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a96fd8-7e6d-42a6-bedd-6d3cf0c244bb",
   "metadata": {},
   "source": [
    "### Q8. What is the Naïve Bayes classifier, and why is it called \"Naïve\"? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b22a224-3ade-4686-a8a7-95f02aab2218",
   "metadata": {},
   "source": [
    "#### A8. The Naive Bayes Classifier is a probabilistic algorithm based on Bayes' Theorem. It assumes that features are independent of each other given the class label, which simplifies the computation of probabilities. The \"naive\" assumption simplifies the likelihood P(X|y) by assuming that all features are conditionally independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db16931-4139-4045-8807-ab314125a109",
   "metadata": {},
   "source": [
    "### Q9. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36aa74-869a-4fbb-92aa-b9cf1d4035d0",
   "metadata": {},
   "source": [
    "#### A9. The Bayesian algorithm is a general probabilistic framework based on Bayes' Theorem, which calculates the conditional probability of an event given prior knowledge. It is expressed as:\n",
    "#### P(A|B) = [P(B|A) * P(A)] / P(B)\n",
    "\n",
    "#### Bernoulli Naive Bayes is a specific implementation of the Naive Bayes classifier designed for binary data. It assumes:\n",
    "#### 1. Features are binary (e.g., presence or absence of a word in text classification).\n",
    "#### 2. Features are conditionally independent given the class label.\n",
    "#### The Bernoulli model uses the Bernoulli distribution, which calculates the probability of success (1) or failure (0) for each feature. The likelihood of a feature is modeled as:\n",
    "#### P(xi|y) = p(i|y)^xi * (1 - p(i|y))^(1 - xi)\n",
    "\n",
    "#### Multinomial Naive Bayes (MNB) is a probabilistic machine learning algorithm based on Bayes' theorem. It is particularly effective for text classification tasks where the features represent discrete frequencies or counts of events, such as word counts in documents.MNB assumes that the features follow a multinomial distribution, which is suitable for data where features are counts or frequencies. The algorithm calculates the probability distribution of text data, making it well-suited for natural language processing (NLP) tasks.\n",
    "#### The term \"multinomial\" refers to the type of data distribution assumed by the model. The features in text classification are typically word counts or term frequencies. The multinomial distribution is used to estimate the likelihood of seeing a specific set of word counts in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d53671-de2a-4c0e-8b4d-acacf6105170",
   "metadata": {},
   "source": [
    "### Q10. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2d6b921-a3be-44cd-a643-1fa60243ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gaussian Naïve Bayes classifier: 0.94\n"
     ]
    }
   ],
   "source": [
    "#A10. \n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize the Gaussian Naïve Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the classifier\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Gaussian Naïve Bayes classifier: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
